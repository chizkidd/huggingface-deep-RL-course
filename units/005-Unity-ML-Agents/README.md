# Unit 5: Introduction to Unity ML-Agents

## Table of Contents
1. [Introduction](#51-introduction)
2. [How ML-Agents Works](#52-how-ml-agents-works)
3. [The SnowballTarget Environment](#53-the-snowball-target-environment)
4. [The Pyramids Environment](#54-the-pyramids-environment)
5. [What is Curiosity?](#55-what-is-curiosity)
    * [Intrinsic Curiosity Module (ICM)](#the-intrinsic-curiosity-module-icm)
    * [Mathematical Derivation](#mathematical-derivation)
6. [Glossary](#56-glossary)

---

## 5.1 Introduction
Unit 5 introduces **Unity ML-Agents**, an open-source plugin that enables game engines (Unity) to serve as environments for training intelligent agents. Instead of simple grids or 2D Atari frames, we can now train in complex 3D environments with physics-based interactions.

Key Benefits:
* **Physics Engine:** Handles collisions, gravity, and complex 3D movement.
* **Custom Environments:** Create specific challenges for agents.
* **External Brain:** Training happens in Python (using PyTorch), while the simulation happens in Unity.

---

## 5.2 How ML-Agents Works
The toolkit uses a **Client-Server architecture** involving three main components:

1.  **Learning Environment:** The Unity scene containing the agents.
2.  **Brain:** The policy that decides which actions to take.
3.  **Academy:** Orchestrates the simulation (resetting scenes, step synchronization).

### The Communication Loop:
* **Unity (C#)** sends observations (images, raycasts) and rewards to Python.
* **Python (PyTorch)** receives data via the `mlagents-learn` wrapper, calculates the next action using a RL algorithm (usually PPO), and sends the action back to Unity.

---

## 5.3 The SnowballTarget Environment
In this environment, an agent learns to shoot snowballs at a moving target.
* **Goal:** Hit the target as many times as possible within a time limit.
* **Observations:** Raycasts (detecting the target and walls) and the agent's rotation.
* **Actions:** Discrete (Turn left/right, Shoot).
* **Rewards:** +1 for hitting the target, -0.01 small time penalty to encourage speed.

---

## 5.4 The Pyramids Environment
A more complex task involving multi-step logic:
1.  Navigate a room.
2.  Find and press a button to spawn a pyramid.
3.  Navigate to the pyramid, knock it over.
4.  Move to the gold brick at the top.

**Problem:** This task has **Sparse Rewards**. The agent only gets a reward at the very end. Without help, the agent might never randomly stumble upon the goal. This is where **Curiosity** comes in.

---

## 5.5 What is Curiosity?
Curiosity is an **Intrinsic Reward** generated by the agent itself to encourage exploration when extrinsic rewards (from the environment) are rare or zero.

### Two Major Problems in RL:
1.  **Sparse Rewards:** Most rewards are zero until the final goal.
2.  **Handmade Rewards:** Human-defined rewards are hard to scale in complex worlds.

### The Intrinsic Curiosity Module (ICM)
The agent receives a reward based on its **inability to predict the consequences of its own actions**.

#### Mathematical Derivation:
The agent has two neural networks in the ICM:
1.  **Forward Model:** Predicts the next state $\hat{s}_{t+1}$ given $s_t$ and $a_t$.
2.  **Inverse Model:** Predicts the action $a_t$ taken to get from $s_t$ to $s_{t+1}$.

The **Intrinsic Reward ($r_i$)** is the error between the predicted next state and the actual next state:

$$r_t = r_e + r_i$$

Where $r_e$ is the extrinsic reward and $r_i$ is calculated as:
$$r_i = \eta \frac{1}{2} \| \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \|^2_2$$

* $\phi(s)$ is the feature representation (embedding) of the state.
* $\eta$ is a scaling factor.
* If the error is **high**, the state is "novel," and the agent gets a "Curiosity Reward."

---

## 5.6 Glossary
* **Observation:** Information the agent receives (Vector for positions, Visual for images).
* **Raycast:** A "laser beam" sent out by the agent to detect distance to objects in 3D space.
* **Intrinsic Reward:** A reward generated by the agent for itself (e.g., Curiosity).
* **Extrinsic Reward:** A reward provided by the environment (e.g., +1 for finishing the level).
* **Sparse Reward:** An environment where rewards are only given at the end of a long sequence of actions.
* **Vector Observation:** A list of numbers representing positions, velocities, or rotations.